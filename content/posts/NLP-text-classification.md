---
title: ç¥ç»ç½‘ç»œç”¨äºæ–‡æœ¬åˆ†ç±»
date: 2018-01-15
mathjax: true
author: "Cong Chan"
tags: ['NLP']
---
## æ–‡æœ¬åˆ†ç±»
æ–‡æœ¬åˆ†ç±»æ˜¯å¾ˆå¤šä¸šåŠ¡é—®é¢˜ä¸­å¹¿æ³›ä½¿ç”¨åˆ°çš„NLP/ç›‘ç£æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ã€‚æ–‡æœ¬åˆ†ç±»çš„ç›®æ ‡æ˜¯è‡ªåŠ¨å°†æ–‡æœ¬/æ–‡æ¡£åˆ†ç±»ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªé¢„å®šä¹‰ç±»åˆ«ã€‚ç›®å‰çš„æˆç†Ÿæ€è·¯æ˜¯ç”¨è¯å‘é‡è§£ç æ–‡æœ¬ï¼Œç„¶åä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹æˆ–è€…æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹æ¥åšåˆ†ç±»ã€‚

æ–‡æœ¬åˆ†ç±»æ˜¯å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéå¸¸æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡ä¸»è¦ä»‹ç»ç”¨äºæ–‡æœ¬åˆ†ç±»çš„å‡ ç§ç¥ç»ç½‘ç»œæ¨¡å‹æ–¹æ³•ï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬çš„æ€§èƒ½ï¼Œä»£ç å®ç°ä¸»è¦åŸºäºKerasã€‚æ–‡ä¸­ä»£ç éƒ½åœ¨è¿™ä¸ª[DeepText](https://github.com/congchan/DeepText)GitHubé¡¹ç›®ä¸­.
<!-- more -->

æ–‡æœ¬åˆ†ç±»çš„ä¸€äº›ç¤ºä¾‹åŒ…æ‹¬ï¼š
1. ä»ç¤¾äº¤åª’ä½“ä¸­äº†è§£å—ä¼—æƒ…ç»ªï¼ˆğŸ˜ ğŸ˜ ğŸ˜¥ï¼‰
2. æ£€æµ‹åƒåœ¾é‚®ä»¶å’Œéåƒåœ¾é‚®ä»¶
3. è‡ªåŠ¨æ ‡è®°å®¢æˆ·æŸ¥è¯¢
4. å°†æ–°é—»æ–‡ç« ğŸ“°åˆ†ç±»ä¸ºé¢„å®šä¹‰ä¸»é¢˜

## ç«¯åˆ°ç«¯æ–‡æœ¬åˆ†ç±»æµæ°´çº¿
ç«¯åˆ°ç«¯æ–‡æœ¬åˆ†ç±»æµæ°´çº¿ç”±ä»¥ä¸‹ç»„ä»¶ç»„æˆï¼š
1. è®­ç»ƒæ–‡æœ¬ï¼šè¾“å…¥æ–‡æœ¬ï¼Œæœ‰ç›‘ç£æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å·²æ ‡æ³¨æ•°æ®æ¥å­¦ä¹ å’Œé¢„æµ‹æ‰€éœ€çš„ç±»ã€‚
2. ç‰¹å¾å‘é‡ï¼šç‰¹å¾å‘é‡æ˜¯ç”¨äºè§£ç è¾“å…¥æ•°æ®ç‰¹å¾çš„ä¿¡æ¯çš„å‘é‡ã€‚
3. æ ‡ç­¾ï¼šé¢„å®šä¹‰çš„ç±»åˆ«/ç±»ï¼Œä½œä¸ºæ¨¡å‹é¢„æµ‹çš„ç›®æ ‡ã€‚
4. ç®—æ³•æ¨¡å‹ï¼šèƒ½å¤Ÿå¤„ç†æ–‡æœ¬åˆ†ç±»çš„ç®—æ³•ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼šCNNï¼ŒRNNï¼ŒHAN, Fasttextï¼‰
5. é¢„æµ‹ï¼šå·²ç»åœ¨å†å²æ•°æ®é›†ä¸Šè®­ç»ƒè¿‡çš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºæ‰§è¡Œæ ‡ç­¾é¢„æµ‹ã€‚

è¿™é‡Œä½¿ç”¨æ±½è½¦æ¶ˆè´¹è€…çš„è¯„æµ‹æ•°æ®é›†ï¼Œåœ¨`tsv`æ–‡ä»¶ä¸­, ç¬¬ä¸€åˆ—æ˜¯åºå·å¯¹æˆ‘ä»¬æ²¡ç”¨, ç¬¬äºŒåˆ—æ˜¯`label(0, 1)`ï¼Œåˆ†åˆ«ä»£è¡¨`ï¼ˆæ¶ˆæï¼Œç§¯æï¼‰`è¯„ä»·ï¼Œç¬¬ä¸‰åˆ—æ˜¯æ–‡æœ¬.
```
1	æ“æ§æ€§èˆ’æœã€æ²¹è€—ä½ï¼Œæ€§ä»·æ¯”é«˜
0	åŠ¨åŠ›çš„ç¡®æœ‰ç‚¹ç‚¹è®©æˆ‘ç›¸ä¿¡äº†upçš„ç¡®æ˜¯ä¸ªä»£æ­¥è½¦è€Œå·²!
1	1ã€‚è½¦çš„å¤–è§‚å¾ˆå–œæ¬¢ã€‚2ã€‚çœæ²¹ï¼Œç°åœ¨ç£¨åˆæœŸ7.3ï¼Œç›¸ä¿¡ä»¥åè¿˜ä¼šä¸‹é™ã€‚
1	å†…é¥°çš„åšå·¥å’Œç”¨æ–™åŒçº§åˆ«åŒä»·ä½æœ€åšé“çš„
0	å‡éœ‡ç³»ç»Ÿå¤ªç¡¬ï¼
```
æ•°æ®å¤„ç†ä½¿ç”¨çš„ç±»ï¼Œå…·ä½“è§[ä»£ç é“¾æ¥](https://github.com/congchan/DeepText/blob/a33fe1b8e895916b26bc658f0a02ac8253291d8a/data_process.py#L29)
```python
class DataProcessor(object):
    """ Base class for data converters for sequence classification data sets.
        helper funcitons [read_tsv, read_text, read_json]
    """
    ...

class SampleProcessor(DataProcessor):
    """ Sample processor for the classification data set.
        Tranform the text to tensor for training
        if use pre-train model, need vocabulary file
        usage:
            process data files
            >>> processer = SampleProcessor(config, )

            provide your own data in list format [train_X, train_Y, test_X, test_Y]
            >>> processer = SampleProcessor(config, data)

    """
```

### è¯å‘é‡
ä½¿ç”¨åŒ…å«å¤–éƒ¨çŸ¥è¯†çš„embeddingè¡¨è¾¾å­—è¯æ˜¯ç›®å‰çš„ä¸»æµæ–¹æ³•ï¼Œç»å…¸çš„å¦‚word2vecï¼ŒGLoVeï¼Œè¾ƒæ–°è¿›çš„ ELMoï¼ŒBERTï¼Œç­‰é¢„è®­ç»ƒå‘é‡ï¼Œé›†æˆäº†å…³äºå•è¯çš„æ–°ä¿¡æ¯ï¼ˆè¯æ±‡å’Œè¯­ä¹‰ï¼‰ï¼Œè¿™äº›ä¿¡æ¯å·²ç»åœ¨éå¸¸å¤§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œæç‚¼ã€‚

åœ¨è¿™é‡Œçš„æ¨¡å‹ï¼Œéƒ½å…è®¸æˆ‘ä»¬ç›´æ¥è½½å…¥å¤–éƒ¨çš„ embedding å‚æ•°ã€‚

ç‰¹åˆ«æ˜¯æä¾›äº†é€šè¿‡é¢„è®­ç»ƒçš„BERTè·å–ä¸­æ–‡å•è¯çš„å‘é‡è¡¨è¾¾çš„æ¥å£. æœ€å¥½æ˜¯ä½¿ç”¨åœ¨è‡ªå·±æ–‡æœ¬ä¸Šfine-tuneè¿‡çš„é¢„è®­ç»ƒBERTæ¨¡å‹.
```python
@staticmethod
def load_bert_embedding(vob_size, emb_size, word2id):
    """ Get bert pre-trained representation,
        for example, pre-trained chinese_L-12_H-768_A-12,
            the hidden_size is 768
    """
    ...
    return rep_matrix
```
è¾“å…¥ä½ çš„è¯æ±‡è¡¨, è¿”å›å„ä¸ªè¯æ±‡å¯¹åº”çš„å‘é‡, ä»¥è¯å…¸å½¢å¼è¿”å›. å†…éƒ¨çš„å·¥ä½œæœºåˆ¶æ˜¯æŠŠæ¯ä¸€ä¸ªå•è¯éƒ½ç”¨æ‹¼æ¥èµ·æ¥, ä¹‹é—´ç”¨BERTçš„å¥å­åˆ†éš”ç¬¦`[SEP]`éš”å¼€. åœ¨è¿”å›çš„token level çš„å‘é‡ä¸­é‡æ–°poolå‡ºå„ä¸ªè¯æ±‡çš„è¡¨è¾¾. è¿™ä¸ªæ–¹æ³•å…·ä½“çš„æ•ˆæœæœ‰å¾…éªŒè¯.

### Fasttextæ–‡æœ¬åˆ†ç±»
Fasttext éå¸¸é€‚åˆå¤„ç†ä¸€äº›æ˜¾è€Œæ˜“è§ï¼Œä¸éœ€è¦æ¨ç†ï¼Œæƒ…å†µæ¯”è¾ƒå•çº¯çš„æ–‡æœ¬åˆ†ç±»é—®é¢˜ã€‚å®ƒå°±æ˜¯ä¸€ä¸ªè¯è¢‹æ¨¡å‹ï¼ŒæŠŠæ–‡æœ¬æ‰€æœ‰å•è¯çš„å‘é‡poolåœ¨ä¸€èµ·ï¼Œå¾—å‡ºæ•´ä¸ªæ–‡æœ¬çš„å‘é‡è¡¨è¾¾ï¼Œè¿™ä¸ªæ–‡æœ¬å‘é‡ä½¿ç”¨softmaxåˆ†ç±»å™¨å¾—å‡ºä¸åŒæ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¸ºäº†æ•æ‰è¯ä¹‹é—´çš„é¡ºåºï¼ŒfasttextåŠ å…¥äº†ngramç‰¹å¾ã€‚è¯¦ç»†æ¨èçœ‹è¿™ä¸¤ç¯‡æ–‡ç« 
1. Enriching Word Vectors with Subword Information, P. Bojanowski, E. Grave, A. Joulin, T. Mikolov
2. Bag of Tricks for Efficient Text Classification, A. Joulin, E. Grave, P. Bojanowski, T. Mikolov
![](/images/fasttext.png "Image taken from the original paper")
[ä»£ç é“¾æ¥](https://github.com/congchan/DeepText/blob/a33fe1b8e895916b26bc658f0a02ac8253291d8a/models.py#L133)
```python
def fasttext(max_length, emb_size, max_words, class_num, pre_train_emb=None):
  """ return single label classification fasttext model
      paper: Bag of Tricks for Efficient Text Classification
      The original paper use average pooling.
      In many Kaggle application, Max Pooling is found to be useful
  """
  input = Input(shape=(max_length,), dtype='int32', name='input')

  embeddings_initializer = 'uniform'
  if pre_train_emb is not None:
      embeddings_initializer = initializers.Constant(pre_train_emb)
  embed_input = Embedding(output_dim=emb_size, dtype='float32', input_dim=max_words + 1,
                          input_length=max_length,
                          embeddings_initializer=embeddings_initializer,
                          trainable=True
                          )(input)

  drop_out_input = Dropout(0.5, name='dropout_layer')(embed_input)
  ave_pool = GlobalAveragePooling1D()(drop_out_input)
  max_pool = GlobalMaxPooling1D()(drop_out_input)
  concat_pool = concatenate([ave_pool, max_pool])
  output = Dense(class_num, activation='softmax', name='output')(concat_pool)
  model = Model(inputs=[input], outputs=output)
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model
```
å¯¹äºä¸­æ–‡æ–‡æœ¬ï¼Œå¦‚æœæ•°æ®é›†ä¸æ˜¯å¾ˆå¹²å‡€çš„è¯ï¼ˆæ¯”å¦‚æœ‰é”™åˆ«å­—ï¼‰ï¼Œè€ƒè™‘ä½¿ç”¨ç‰¹æ®Šè¶…å‚çš„fasttextã€‚ä¸€èˆ¬æ¥è¯´fasttextåœ¨è‹±æ–‡ä¸­çš„`char+ngram`çš„çª—å£å¤§å°ä¸€èˆ¬å–å€¼`3 ~ 6`ï¼Œä½†æ˜¯åœ¨å¤„ç†ä¸­æ–‡æ—¶ï¼Œä¸ºäº†å»é™¤è¾“å…¥ä¸­çš„å™ªå£°ï¼Œé‚£ä¹ˆå¯ä»¥æŠŠè¿™ä¸ªçª—å£é™åˆ¶ä¸º`1~2`ï¼Œå› ä¸ºå°çª—å£æœ‰åˆ©äºæ¨¡å‹å»æ•è·**é”™åˆ«å­—**ï¼ˆé”™è¯¯è¯ä¸€èˆ¬éƒ½æ˜¯å…¶ä¸­çš„ä¸€ä¸ªå­—è¡¨è¾¾æˆåŒéŸ³å¼‚å½¢çš„å¦ä¸€ä¸ªå­—ï¼‰ï¼Œæ¯”å¦‚å°ngramçª—å£fasttextå­¦å‡ºæ¥çš„`"ä¼¼ä¹"`è¿‘ä¼¼è¯å¾ˆæœ‰å¯èƒ½åŒ…å«`"æ˜¯ä¹"`ç­‰å†…éƒ¨åŒ…å«é”™åˆ«å­—çš„è¯ï¼Œè¿™æ ·ç­‰äºè®©fasttextæ‹¥æœ‰äº†è¯†åˆ«é”™åˆ«å­—çš„è¯çš„èƒ½åŠ›ã€‚

### å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ–‡æœ¬åˆ†ç±»
CNNé€šå¸¸ç”¨äºè®¡ç®—æœºè§†è§‰ï¼Œä½†å®ƒä»¬æœ€è¿‘å·²åº”ç”¨äºå„ç§NLPä»»åŠ¡ï¼Œç»“æœå¾ˆæœ‰å‰æ™¯ã€‚

ç®€è¦åœ°è¯´ï¼Œåœ¨æ–‡æœ¬æ•°æ®ä¸Šä½¿ç”¨CNNæ—¶ï¼Œå½“æ£€æµ‹åˆ°ç‰¹æ®Šçš„ patternï¼Œæ¯ä¸ªå·ç§¯çš„ç»“æœéƒ½å°†è§¦å‘ã€‚é€šè¿‡æ”¹å˜å†…æ ¸çš„å¤§å°å¹¶è¿æ¥å®ƒä»¬çš„è¾“å‡ºï¼Œä½ å¯ä»¥è‡ªå·±æ£€æµ‹å¤šä¸ªå¤§å°çš„æ¨¡å¼ï¼ˆ2, 3æˆ–5ä¸ªç›¸é‚»çš„å•è¯ï¼‰ã€‚Patterns å¯ä»¥æ˜¯è¡¨è¾¾å¼ï¼ˆå¦‚ ngramsï¼‰ï¼Œå› æ­¤CNNå¯ä»¥åœ¨å¥å­ä¸­è¯†åˆ«å®ƒä»¬è€Œä¸ç®¡å®ƒä»¬çš„ä½ç½®å¦‚ä½•ã€‚
![](/images/textCNN.png "Image Reference : Understanding convolutional neural networks for nlp")

å‚æ•°ä½¿ç”¨ 128 ä¸ª filtersï¼Œå¤§å°ä»1åˆ°4ã€‚æ¨¡å‹æ¶æ„å¦‚å›¾![](/images/textCNNarch.png "reference from https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f")
[ä»£ç é“¾æ¥](https://github.com/congchan/DeepText/blob/a33fe1b8e895916b26bc658f0a02ac8253291d8a/models.py#L161)
```python
def text_cnn(max_length, emb_size, max_words, class_num, pre_train_emb=None):
  " textCNN model "
  ...
  cnn1_1    = Conv1D(128, 1, padding='same', strides=1)(drop_out_layer)
  ...
  cnn1      = GlobalMaxPooling1D()(cnn1_2_at)

  cnn2_1    = Conv1D(128, 2, padding='same', strides=1)(drop_out_layer)
  ...
  cnn2      = GlobalMaxPooling1D()(cnn2_2_at)

  cnn3_1    = Conv1D(128, 4, padding='same', strides=1)(drop_out_layer)
  ...
  cnn3      = GlobalMaxPooling1D()(cnn3_2_at)

  concat_cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)
  ...
  return model
```
ç”¨äºtextçš„CNNä¸ä»…æ›´å®¹æ˜“å¹¶è¡ŒåŒ–è¿ç®—ï¼Œè€Œä¸”å¾ˆå®¹æ˜“æˆä¸ºä¸€ä¸ªæ•°æ®é›†ä¸Šçš„å¾ˆå¼ºçš„baselineï¼ˆé™¤éè¿™ä¸ªåˆ†ç±»ä»»åŠ¡å¾ˆéš¾ï¼‰ã€‚æ ¹æ®æ•°æ®çš„æƒ…å†µé€‰æ‹©æ¨¡å‹ï¼Œå¦‚æœngramç‰¹å¾å¾ˆé‡è¦ï¼Œä½¿ç”¨textCNNï¼Œå¦‚æœæ–‡æœ¬é•¿è·ç¦»ä¾èµ–æ¯”è¾ƒæ˜æ˜¾ï¼Œè€ƒè™‘ä½¿ç”¨RNNã€‚

### RNNç”¨äºæ–‡æœ¬åˆ†ç±»
RNNç”¨äºæ–‡æœ¬åˆ†ç±»çš„è¯ï¼Œseq2one æ¶æ„ï¼ŒæŠŠä¸å®šé•¿åºåˆ—è§£ç ä¸ºå®šé•¿å‘é‡ï¼Œå†æŠŠè¿™ä¸ªè¾“å‡ºå‘é‡ç”¨softmaxå‡½æ•°è®¡ç®—å‡ºå„æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚RNN(LSTM/GRU)å› ä¸ºå¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›è¾ƒå¼±ï¼Œç›®å‰ä¸€èˆ¬éœ€è¦åŠ ä¸Šæ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™é‡Œæš‚æ—¶ç®€å•ç²—æš´çš„ç”¨åŒå‘GRUæ¥å®šä¹‰æ ¸å¿ƒçš„encoder.
```python
def text_rnn(max_length, emb_size, max_words, class_num, pre_train_emb=None):
    " Text RNN model using GRU cell"
    return _bilstm_attention(max_length, emb_size, max_words, class_num, False, pre_train_emb)

def text_rnn_attention(max_length, emb_size, max_words, class_num, pre_train_emb=None):
  " Text RNN model using GRU cell with attention mechanism"
  return _bilstm_attention(max_length, emb_size, max_words, class_num, True, pre_train_emb)
```

#### RCNN

### Hierarchical Attention Network (HAN)

## Reference
Enriching Word Vectors with Subword Information, P. Bojanowski, E. Grave, A. Joulin, T. Mikolov
Bag of Tricks for Efficient Text Classification, A. Joulin, E. Grave, P. Bojanowski, T. Mikolov
https://arxiv.org/abs/1408.5882 Yoon Kim
http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ by Denny Britz.
Understanding convolutional neural networks for nlp
https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f
[Hierarchical Attention Networks for Document Classification ](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)
